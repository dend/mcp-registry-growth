# CSV Data Contract

**File**: `data/mcp-servers.csv`  
**Generated By**: GitHub Actions PowerShell script  
**Updated**: Hourly  
**Format**: CSV with header row

## Schema Definition

### Header Row
```csv
timestamp,local_count,remote_count,total_count,unique_count,collection_time
```

### Field Specifications

| Field | Type | Format | Description | Constraints |
|-------|------|--------|-------------|-------------|
| `timestamp` | ISO8601 | `YYYY-MM-DDTHH:00:00.000Z` | Data point timestamp (hourly boundary) | Must align to hour boundaries |
| `local_count` | Integer | `>=0` | Count of local servers (packages only or neither) | Non-negative |
| `remote_count` | Integer | `>=0` | Count of remote servers (remotes only) | Non-negative |
| `total_count` | Integer | `>=0` | Sum including "both" servers counted in each category | `>= max(local_count, remote_count)` |
| `unique_count` | Integer | `>=0` | Each server counted exactly once | `<= total_count` |
| `collection_time` | ISO8601 | `YYYY-MM-DDTHH:MM:SS.sssZ` | Actual time API was called | Within 10 minutes of timestamp |

### Sample Data
```csv
timestamp,local_count,remote_count,total_count,unique_count,collection_time
2025-09-16T14:00:00.000Z,150,75,235,200,2025-09-16T14:05:32.123Z
2025-09-16T15:00:00.000Z,152,76,238,203,2025-09-16T15:05:28.456Z
2025-09-16T16:00:00.000Z,154,78,242,206,2025-09-16T16:05:31.789Z
```

## Data Validation Rules

### Required Validations
1. **Header Validation**: First row must match exact header format
2. **Field Count**: Each row must have exactly 6 fields
3. **Data Types**: All numeric fields must be valid integers
4. **Timestamp Format**: ISO8601 format validation for timestamp fields
5. **Logical Consistency**: `total_count >= local_count` and `total_count >= remote_count`
6. **Unique Count Logic**: `unique_count <= total_count`
7. **Chronological Order**: Timestamps should be in ascending order
8. **Time Alignment**: Timestamps must be on exact hour boundaries

### Error Handling
- **Invalid Row**: Skip row, log warning, continue processing
- **Missing File**: Use empty dataset, show error state in UI
- **Corrupted File**: Attempt to parse valid rows, alert monitoring
- **Large File**: Truncate to most recent 8760 hours (1 year) for performance

## Usage Contract

### Build-Time Loading
```typescript
// In lib/data.ts
import fs from 'fs';
import { parse } from 'csv-parse/sync';

interface CSVDataPoint {
  timestamp: string;
  local_count: string;
  remote_count: string;
  total_count: string;
  unique_count: string;
  collection_time: string;
}

export function loadAnalyticsData(): AnalyticsDataPoint[] {
  const csvPath = path.join(process.cwd(), 'data', 'mcp-servers.csv');
  
  if (!fs.existsSync(csvPath)) {
    console.warn('CSV data file not found, using empty dataset');
    return [];
  }
  
  const csvContent = fs.readFileSync(csvPath, 'utf-8');
  const rawData: CSVDataPoint[] = parse(csvContent, {
    columns: true,
    skip_empty_lines: true,
    trim: true
  });
  
  return rawData.map(row => ({
    timestamp: row.timestamp,
    localCount: parseInt(row.local_count, 10),
    remoteCount: parseInt(row.remote_count, 10),
    totalCount: parseInt(row.total_count, 10),
    uniqueCount: parseInt(row.unique_count, 10),
    granularity: 'hourly' as const
  }));
}
```

### PowerShell Generation Contract
```powershell
# Expected output format from Collect-MCPData.ps1
$csvRow = @{
    timestamp = (Get-Date).ToString("yyyy-MM-ddTHH:00:00.000Z")
    local_count = $localServers.Count
    remote_count = $remoteServers.Count  
    total_count = $localServers.Count + $remoteServers.Count + $bothServers.Count
    unique_count = $allServers.Count
    collection_time = (Get-Date).ToString("yyyy-MM-ddTHH:mm:ss.fffZ")
}
```

## Performance Considerations

### File Size Limits
- **Maximum Size**: 10MB for efficient Next.js build performance
- **Row Limit**: Approximately 8760 rows (1 year of hourly data)
- **Archival Strategy**: Move older data to separate archive files if needed

### Loading Optimization
- **Build-Time Only**: CSV parsing happens only during Next.js build
- **Memory Usage**: Entire file loaded into memory during build (acceptable for <10MB)
- **Caching**: Processed data cached in Next.js build artifacts

## Monitoring & Alerts

### Data Quality Monitoring
- **Missing Updates**: Alert if no new data for >2 hours
- **Data Anomalies**: Alert if count decreases by >50% in single hour
- **File Corruption**: Alert if CSV parsing fails
- **Stale Data**: Warning if collection_time is >1 hour behind timestamp

### GitHub Actions Integration
- **Success Metrics**: Track successful vs failed data collection runs
- **Error Reporting**: Log detailed errors for failed API calls
- **Commit History**: Each data update creates a Git commit for auditability